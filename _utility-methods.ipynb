{"cells":[{"cell_type":"code","source":["# Debugging\n# username = spark.sql(\"SELECT current_user()\").first()[0]\n# path = f\"dbfs:/user/{username}/dbacademy/dewd/\"\n# print(f\"Nuking {path}\")\n# dbutils.fs.rm(path, True)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c9a86de8-ce1e-492a-9e1b-a10920f1c2e7","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["class Paths():\n    def __init__(self, working_dir, clean_lesson):\n        self.working_dir = working_dir\n\n        if clean_lesson: self.user_db = f\"{working_dir}/{clean_lesson}.db\"\n        else:            self.user_db = f\"{working_dir}/database.db\"\n            \n    def exists(self, path):\n        try: return len(dbutils.fs.ls(path)) >= 0\n        except Exception:return False\n\n    def print(self, padding=\"  \"):\n        max_key_len = 0\n        for key in self.__dict__: max_key_len = len(key) if len(key) > max_key_len else max_key_len\n        for key in self.__dict__:\n            label = f\"{padding}DA.paths.{key}:\"\n            print(label.ljust(max_key_len+13) + DA.paths.__dict__[key])\n\nclass DBAcademyHelper():\n    def __init__(self, lesson=None):\n        import re, time\n\n        self.start = int(time.time())\n        \n        self.course_name = \"dewd\"\n        self.lesson = lesson.lower()\n        self.data_source_uri = \"wasbs://courseware@dbacademy.blob.core.windows.net/data-engineering-with-databricks/v02\"\n\n        # Define username\n        self.username = spark.sql(\"SELECT current_user()\").first()[0]\n        self.clean_username = re.sub(\"[^a-zA-Z0-9]\", \"_\", self.username)\n\n        self.db_name_prefix = f\"dbacademy_{self.clean_username}_{self.course_name}\"\n        self.source_db_name = None\n\n        self.working_dir_prefix = f\"dbfs:/user/{self.username}/dbacademy/{self.course_name}\"\n        \n        if self.lesson:\n            clean_lesson = re.sub(\"[^a-zA-Z0-9]\", \"_\", self.lesson)\n            working_dir = f\"{self.working_dir_prefix}/{self.lesson}\"\n            self.paths = Paths(working_dir, clean_lesson)\n            self.hidden = Paths(working_dir, clean_lesson)\n            self.db_name = f\"{self.db_name_prefix}_{clean_lesson}\"\n        else:\n            working_dir = self.working_dir_prefix\n            self.paths = Paths(working_dir, None)\n            self.hidden = Paths(working_dir, None)\n            self.db_name = self.db_name_prefix\n\n    def init(self, create_db=True):\n        spark.catalog.clearCache()\n        self.create_db = create_db\n        \n        if create_db:\n            print(f\"\\nCreating the database \\\"{self.db_name}\\\"\")\n            spark.sql(f\"CREATE DATABASE IF NOT EXISTS {self.db_name} LOCATION '{self.paths.user_db}'\")\n            spark.sql(f\"USE {self.db_name}\")\n\n    def cleanup(self):\n        for stream in spark.streams.active:\n            print(f\"Stopping the stream \\\"{stream.name}\\\"\")\n            stream.stop()\n            try: stream.awaitTermination()\n            except: pass # Bury any exceptions\n\n        if spark.sql(f\"SHOW DATABASES\").filter(f\"databaseName == '{self.db_name}'\").count() == 1:\n            print(f\"Dropping the database \\\"{self.db_name}\\\"\")\n            spark.sql(f\"DROP DATABASE {self.db_name} CASCADE\")\n            \n        if self.paths.exists(self.paths.working_dir):\n            print(f\"Removing the working directory \\\"{self.paths.working_dir}\\\"\")\n            dbutils.fs.rm(self.paths.working_dir, True)\n\n    def conclude_setup(self):\n        import time\n\n        spark.conf.set(\"da.username\", self.username)\n        spark.conf.set(\"da.db_name\", self.db_name)\n        for key in self.paths.__dict__:\n            spark.conf.set(f\"da.paths.{key.lower()}\", self.paths.__dict__[key])\n\n        print(\"\\nPredefined Paths:\")\n        DA.paths.print()\n\n        if self.source_db_name:\n            print(f\"\\nPredefined tables in {self.source_db_name}:\")\n            tables = spark.sql(f\"SHOW TABLES IN {self.source_db_name}\").filter(\"isTemporary == false\").select(\"tableName\").collect()\n            if len(tables) == 0: print(\"  -none-\")\n            for row in tables: print(f\"  {row[0]}\")\n\n        if self.create_db:\n            print(f\"\\nPredefined tables in {self.db_name}:\")\n            tables = spark.sql(f\"SHOW TABLES IN {self.db_name}\").filter(\"isTemporary == false\").select(\"tableName\").collect()\n            if len(tables) == 0: print(\"  -none-\")\n            for row in tables: print(f\"  {row[0]}\")\n                \n        print(f\"\\nSetup completed in {int(time.time())-self.start} seconds\")\n        \n    def block_until_stream_is_ready(self, query, min_batches=2):\n        import time\n        while len(query.recentProgress) < min_batches:\n            time.sleep(5) # Give it a couple of seconds\n\n        print(f\"The stream has processed {len(query.recentProgress)} batchs\")\n        \ndbutils.widgets.text(\"lesson\", \"missing\")\nlesson = dbutils.widgets.get(\"lesson\")\nif lesson == \"none\": lesson = None\nassert lesson != \"missing\", f\"The lesson must be passed to the DBAcademyHelper\"\n\nDA = DBAcademyHelper(lesson)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"59314dc1-4f99-46bb-8b76-a8ffedc5b6bd","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def install_source_dataset(source_uri, reinstall, subdir):\n    target_dir = f\"{DA.working_dir_prefix}/source/{subdir}\"\n\n#     if reinstall and DA.paths.exists(target_dir):\n#         print(f\"Removing existing dataset at {target_dir}\")\n#         dbutils.fs.rm(target_dir, True)\n    \n    if DA.paths.exists(target_dir):\n        print(f\"Skipping install to \\\"{target_dir}\\\", dataset already exists\")\n    else:\n        print(f\"Installing datasets to \\\"{target_dir}\\\"\")\n        dbutils.fs.cp(source_uri, target_dir, True)\n        \n    return target_dir"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0b085b9d-fdcb-46cc-b59e-8f8127d13578","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def install_dtavod_datasets(reinstall):\n    source_uri = \"wasbs://courseware@dbacademy.blob.core.windows.net/databases_tables_and_views_on_databricks/v02\"\n    DA.paths.datasets = install_source_dataset(source_uri, reinstall, \"dtavod\")\n\n    copy_source_dataset(f\"{DA.paths.datasets}/flights/departuredelays.csv\", \n                        f\"{DA.paths.working_dir}/flight_delays\",\n                        format=\"csv\", name=\"flight_delays\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7a4e01f7-2946-4883-9094-9aae67c754db","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def install_eltwss_datasets(reinstall):\n    source_uri = \"wasbs://courseware@dbacademy.blob.core.windows.net/elt-with-spark-sql/v02/small-datasets\"\n    DA.paths.datasets = install_source_dataset(source_uri, reinstall, \"eltwss\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fc109ab5-df3e-4924-8469-0d1655319fbf","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def clone_source_table(table_name, source_path, source_name=None):\n    import time\n    start = int(time.time())\n\n    source_name = table_name if source_name is None else source_name\n    print(f\"Cloning the {table_name} table from {source_path}/{source_name}\", end=\"...\")\n    \n    spark.sql(f\"\"\"\n        CREATE OR REPLACE TABLE {table_name}\n        SHALLOW CLONE delta.`{source_path}/{source_name}`\n        \"\"\")\n\n    total = spark.read.table(table_name).count()\n    print(f\"({int(time.time())-start} seconds / {total:,} records)\")\n    \ndef load_eltwss_tables():\n    clone_source_table(\"events\", f\"{DA.paths.datasets}/delta\")\n    clone_source_table(\"sales\", f\"{DA.paths.datasets}/delta\")\n    clone_source_table(\"users\", f\"{DA.paths.datasets}/delta\")\n    clone_source_table(\"transactions\", f\"{DA.paths.datasets}/delta\")    "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"99cb528c-3ea0-4a32-8162-811b1a8ba53c","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def copy_source_dataset(src_path, dst_path, format, name):\n    import time\n    start = int(time.time())\n    print(f\"Creating the {name} dataset\", end=\"...\")\n    \n    dbutils.fs.cp(src_path, dst_path, True)\n\n    total = spark.read.format(format).load(dst_path).count()\n    print(f\"({int(time.time())-start} seconds / {total:,} records)\")\n    \ndef load_eltwss_external_tables():\n    copy_source_dataset(f\"{DA.paths.datasets}/raw/sales-csv\", \n                        f\"{DA.paths.working_dir}/sales-csv\", \"csv\", \"sales-csv\")\n\n    import time\n    start = int(time.time())\n    print(f\"Creating the users table\", end=\"...\")\n\n    # REFACTORING - Making lesson-specific copy\n    dbutils.fs.cp(f\"{DA.paths.datasets}/raw/users-historical\", \n                  f\"{DA.paths.working_dir}/users-historical\", True)\n\n    # https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html\n    (spark.read\n          .format(\"parquet\")\n          .load(f\"{DA.paths.working_dir}/users-historical\")\n          .repartition(1)\n          .write\n          .format(\"org.apache.spark.sql.jdbc\")\n          .option(\"url\", f\"jdbc:sqlite:/{DA.username}_ecommerce.db\")\n          .option(\"dbtable\", \"users\") # The table name in sqllight\n          .mode(\"overwrite\")\n          .save())\n\n    total = spark.read.parquet(f\"{DA.paths.working_dir}/users-historical\").count()\n    print(f\"({int(time.time())-start} seconds / {total:,} records)\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"121b1bd0-137b-48a4-a493-30f0dd2b0601","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# lesson: Writing delta \ndef create_eltwss_users_update():\n    import time\n    start = int(time.time())\n    print(f\"Creating the users_dirty table\", end=\"...\")\n\n    # REFACTORING - Making lesson-specific copy\n    dbutils.fs.cp(f\"{DA.paths.datasets}/raw/users-30m\", \n                  f\"{DA.paths.working_dir}/users-30m\", True)\n    \n    spark.sql(f\"\"\"\n        CREATE OR REPLACE TABLE users_dirty AS\n        SELECT *, current_timestamp() updated \n        FROM parquet.`{DA.paths.working_dir}/users-30m`\n    \"\"\")\n    \n    spark.sql(\"INSERT INTO users_dirty VALUES (NULL, NULL, NULL, NULL), (NULL, NULL, NULL, NULL), (NULL, NULL, NULL, NULL)\")\n    \n    total = spark.read.table(\"users_dirty\").count()\n    print(f\"({int(time.time())-start} seconds / {total:,} records)\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c0067e88-007b-443a-9bfb-2934e1e536fa","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["class DltDataFactory:\n    def __init__(self):\n        self.source = f\"/mnt/training/healthcare/tracker/streaming\"\n        self.userdir = f\"{DA.paths.working_dir}/source/tracker\"\n        try:\n            self.curr_mo = 1 + int(max([x[1].split(\".\")[0] for x in dbutils.fs.ls(self.userdir)]))\n        except:\n            self.curr_mo = 1\n    \n    def load(self, continuous=False):\n        if self.curr_mo > 12:\n            print(\"Data source exhausted\\n\")\n        elif continuous == True:\n            while self.curr_mo <= 12:\n                curr_file = f\"{self.curr_mo:02}.json\"\n                target_dir = f\"{self.userdir}/{curr_file}\"\n                print(f\"Loading the file {curr_file} to the {target_dir}\")\n                dbutils.fs.cp(f\"{self.source}/{curr_file}\", target_dir)\n                self.curr_mo += 1\n        else:\n            curr_file = f\"{str(self.curr_mo).zfill(2)}.json\"\n            target_dir = f\"{self.userdir}/{curr_file}\"\n            print(f\"Loading the file {curr_file} to the {target_dir}\")\n\n            dbutils.fs.cp(f\"{self.source}/{curr_file}\", target_dir)\n            self.curr_mo += 1\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f512835b-5cb6-4e33-8359-eaf5d2b7af06","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"_utility-methods","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":827697314580229}},"nbformat":4,"nbformat_minor":0}
